{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create the confusion matrix\n",
    "def confusion_matrix(y,y_pred):\n",
    "    conf_matrix=np.zeros((2,2))\n",
    "    classes=np.unique(y)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            conf_matrix[i,j]=np.sum((y==classes[i]) & (y_pred ==classes[j]))\n",
    "    return conf_matrix\n",
    "\n",
    "### function to create the accuracy score based i=on confusion matrix\n",
    "def accuracy_score(conf_matrix):\n",
    "    total=np.sum(conf_matrix)\n",
    "    total_tn_tp=np.trace(conf_matrix)\n",
    "    accuracy=total_tn_tp/total\n",
    "    print(\"Accuracy score is\",accuracy)\n",
    "    return\n",
    "\n",
    "### function to create the f1 score\n",
    "def f1_score(conf_matrix):    \n",
    "    tot_positive=conf_matrix.item((1,1))\n",
    "    tp_fp=conf_matrix.item((1,0))+conf_matrix.item((1,1))\n",
    "    precision=tot_positive/tp_fp\n",
    "    print(\"Precision is \",precision)\n",
    "    \n",
    "    tp_fn=conf_matrix.item((0,1))+conf_matrix.item((1,1))\n",
    "    recall=tot_positive/tp_fn\n",
    "    print(\"Recall is \",recall)\n",
    "    \n",
    "    f1=2*((precision*recall)/(precision+recall))\n",
    "    print(\"F1 Score is \" ,f1)\n",
    "    return \n",
    "\n",
    "### function to create the auc score\n",
    "def auc_score(y,y_prob,thresholds):\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        y_pred = np.where(y_prob >= threshold, 1, 0)\n",
    "        #y_pred=np.sort(np.array(y_pred))[::-1]\n",
    "        \n",
    "        fp = np.sum((y_pred == 1) & (y == 0))\n",
    "        tp = np.sum((y_pred == 1) & (y == 1))\n",
    "\n",
    "        fn = np.sum((y_pred == 0) & (y == 1))\n",
    "        tn = np.sum((y_pred == 0) & (y == 0))\n",
    "       \n",
    "        fpr.append(fp / (fp + tn))\n",
    "        tpr.append(tp / (tp + fn))\n",
    "\n",
    "        #auc=np.trapz(tpr,fpr)\n",
    "   # print(auc)\n",
    "    return (tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the 5_a csv file into data frame df_a\n",
    "df_a=pd.read_csv(\"5_a.csv\")\n",
    "df_a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba  y_pred\n",
       "0  1.0  0.637387       1\n",
       "1  1.0  0.635165       1\n",
       "2  1.0  0.766586       1\n",
       "3  1.0  0.724564       1\n",
       "4  1.0  0.889199       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### calculated the class labels based on the condition and stored in 'y_pred' column\n",
    "df_a['y_pred']= df_a.apply(lambda x: 1 if x.proba >0.5 else 0,axis=1)\n",
    "df_a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stores the columns of dataframe in individual colums  \n",
    "y=list(df_a['y'])\n",
    "y_pred=list(df_a['y_pred'])\n",
    "y_prob=list(df_a['proba'])\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is\n",
      "[[    0.   100.]\n",
      " [    0. 10000.]]\n",
      "Accuracy score is 0.9900990099009901\n",
      "Precision is  1.0\n",
      "Recall is  0.9900990099009901\n",
      "F1 Score is  0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "### calculated the confusion matrix. accuracy score, recall, precision and f1 score by calling respective functions.\n",
    "conf_matrix=confusion_matrix(y,y_pred)\n",
    "print(\"Confusion matrix is\")\n",
    "print(conf_matrix)\n",
    "accuracy_score(conf_matrix)\n",
    "f1_score(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort the vales of proba colums and stored in threshold variable in descending order\n",
    "thresholds=np.sort(np.array(df_a['proba']))[::-1]\n",
    "#print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selected the respetcive columns and stored in the variable y1,y_prob1\n",
    "y1=df_a['y']\n",
    "y_prob1=df_a['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculated the auc score based on fpr,tpr\n",
    "tpr,fpr=auc_score(y1,y_prob1,thresholds)\n",
    "#auc_score(y1,y_prob1,thresholds)\n",
    "auc=np.trapz(tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score is :  0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score is : \",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  0.0  0.281035\n",
       "1  0.0  0.465152\n",
       "2  0.0  0.352793\n",
       "3  0.0  0.157818\n",
       "4  0.0  0.276648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the 5_b csv file into data frame df_b\n",
    "df_b=pd.read_csv(\"C:\\\\Users\\\\poonam\\\\Desktop\\\\FIU\\\\Applied AI Course\\\\Assignment 5\\\\5_b.csv\")\n",
    "df_b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculated the class labels based on the condition and stored in 'y_pred' column\n",
    "df_b['y_pred']= df_b.apply(lambda x: 1 if x.proba >0.5 else 0,axis=1)\n",
    "#df_b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stores the columns of dataframe in individual variables \n",
    "yb=df_b['y']\n",
    "yb_pred=df_b['y_pred']\n",
    "yb_prob=df_b['proba']\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is\n",
      "[[9761.  239.]\n",
      " [  45.   55.]]\n",
      "Accuracy score is 0.9718811881188119\n",
      "Precision is  0.55\n",
      "Recall is  0.1870748299319728\n",
      "F1 Score is  0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "### calculated the confusion matrix. accuracy score, recall, precision and f1 score by calling respective functions.\n",
    "conf_matrix_b=confusion_matrix(yb,yb_pred)\n",
    "print(\"Confusion matrix is\")\n",
    "print(conf_matrix_b)\n",
    "\n",
    "accuracy_score(conf_matrix_b)\n",
    "f1_score(conf_matrix_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort the vales of proba colums and stored in threshold variable in descending order\n",
    "thresholds_b=np.sort(np.array(df_b['proba']))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selected the respetcive columns and stored in the variable y1,y_prob1\n",
    "yb1=df_b['y']\n",
    "yb_prob1=df_b['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculated the auc score based on fpr,tpr\n",
    "#auc_score(yb1,yb_prob1,thresholds_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score is :  0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "### calculated the auc score based on fpr,tpr\n",
    "tpr,fpr=auc_score(yb1,yb_prob1,thresholds_b)\n",
    "#auc_score(y1,y_prob1,thresholds)\n",
    "auc=np.trapz(tpr,fpr)\n",
    "print(\"AUC Score is : \",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('5_c.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141000\n",
      "Best Threshold is:  [0.2300390278970873]\n"
     ]
    }
   ],
   "source": [
    "dict_matrix_A={}\n",
    "\n",
    "unique_prob_score=data['prob'].unique()\n",
    "for i in unique_prob_score:\n",
    "    data['y_pred']=np.where(data['prob']>=i,1,0)\n",
    "    FP=((data['y']==0.0)&(data['y_pred']==1.0)).sum()\n",
    "    FN=((data['y']==1.0)&(data['y_pred']==0.0)).sum()\n",
    "    A=((500*FN)+(100*FP))\n",
    "    dict_matrix_A[i]=A\n",
    "least_A=min(dict_matrix_A.values())\n",
    "print(least_A)\n",
    "best_thresholds=[threshold for threshold,m_A in dict_matrix_A.items() if m_A==least_A ]\n",
    "print(\"Best Threshold is: \",best_thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create mean square error\n",
    "def mean_sqaure_error(y,y_pred):\n",
    "    mse=np.square(np.subtract(y,y_pred)).mean()\n",
    "    print(\"Mean Sqaure Error is \",mse )\n",
    "    return\n",
    "\n",
    "### function to create mean absolute error\n",
    "def mean_absolute_per_error(y,y_pred):\n",
    "    mape= np.mean(np.abs((y_pred-y) /np.mean(y))) \n",
    "    print(\"Modified Mean absolute percentage Error is \",mape )\n",
    "    return\n",
    "\n",
    "### function to create r2 score\n",
    "def r2_score(y,y_pred):\n",
    "\n",
    "    ymean=np.mean(y)\n",
    "    #print(ymean)\n",
    "    sstotal=np.square(np.subtract(y,ymean)).sum()\n",
    "    #print(sstotal)\n",
    "\n",
    "    ssresidual=np.square(np.subtract(y,y_pred)).sum()\n",
    "    #print(ssresidual)\n",
    "\n",
    "    r2=1-(ssresidual/sstotal)\n",
    "    print(\"r2 score is \",r2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the csv file into data frame\n",
    "df_d=pd.read_csv(\"5_d.csv\")\n",
    "#df_d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the indicidual columns into varaiable\n",
    "y=df_d['y']\n",
    "y_pred=df_d['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Sqaure Error is  177.16569974554707\n",
      "Modified Mean absolute percentage Error is  0.12912029940096315\n",
      "r2 score is  0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "### call the functions \n",
    "mean_sqaure_error(y,y_pred)\n",
    "mean_absolute_per_error(y,y_pred)\n",
    "r2_score(y,y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
